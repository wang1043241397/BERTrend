#  Copyright (c) 2024, RTE (https://www.rte-france.com)
#  See AUTHORS.txt
#  SPDX-License-Identifier: MPL-2.0
#  This file is part of BERTrend.

import ast
import datetime

import streamlit as st
from loguru import logger
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer

from bertrend import DATA_PATH, OUTPUT_PATH
from bertrend.demos.demos_utils.data_loading_component import (
    display_data_loading_component,
)

from bertrend.demos.topic_analysis.app_utils import (
    embedding_model_options,
    bertopic_options,
    umap_options,
    hdbscan_options,
    countvectorizer_options,
    ctfidf_options,
    representation_model_options,
    load_data_wrapper,
)
from bertrend.demos.topic_analysis.data_utils import data_overview, choose_data
from bertrend.demos.demos_utils.state_utils import (
    register_widget,
    save_widget_state,
    restore_widget_state,
)

from bertrend.metrics.topic_metrics import get_coherence_value, get_diversity_value
from bertrend.parameters import BERTOPIC_SERIALIZATION
from bertrend.train import train_BERTopic
from bertrend.utils.data_loading import (
    split_df_by_paragraphs,
    TIMESTAMP_COLUMN,
    URL_COLUMN,
    TEXT_COLUMN,
    preprocess_french_text,
    clean_dataset,
)


def split_dataframe(split_option, enhanced):
    """
    Split the dataframe based on the selected option.

    Args:
    split_option (str): The selected split option ('No split', 'Split by paragraphs')
    enhanced (bool): Whether to use enhanced splitting. Useful if we want to guarantee avoiding truncation
    during the embedding process, which happens if the input sequence length is more than the embedding model
    could handle.
    """
    if split_option == "No split":
        st.session_state["split_df"] = st.session_state["raw_df"]
        st.session_state["split_by_paragraphs"] = False
    else:  # Split by paragraph
        if enhanced:
            logger.debug(
                f"Using {st.session_state.get('embedding_model_name')} for enhanced splitting..."
            )
            model_name = st.session_state.get("embedding_model_name")
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            max_length = SentenceTransformer(model_name).get_max_seq_length()

            # Correcting the max seq length anomaly in certain embedding models description
            if max_length == 514:
                max_length = 512

            with st.spinner("Splitting the dataset..."):
                st.session_state["split_df"] = split_df_by_paragraphs(
                    dataset=st.session_state["raw_df"],
                    enhanced=True,
                    tokenizer=tokenizer,
                    max_length=max_length
                    - 2,  # Minus 2 because beginning and end tokens are not considered
                    min_length=0,
                )
        else:
            st.session_state["split_df"] = split_df_by_paragraphs(
                st.session_state["raw_df"], enhanced=False
            )
        st.session_state["split_by_paragraphs"] = True


def generate_model_name(base_name="topic_model"):
    """
    Generates a dynamic model name with the current date and time.
    If a base name is provided, it uses that instead of the default.
    """
    current_datetime = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    model_name = f"{base_name}_{current_datetime}"
    return model_name


def save_model_interface():
    st.write("## Save Model")

    # Optional text box for custom model name
    base_model_name = st.text_input(
        "Enter a name for the model (optional):", key="base_model_name_input"
    )

    # Button to save the model
    if st.button("Save Model", key="save_model_button"):
        if "topic_model" in st.session_state:
            dynamic_model_name = generate_model_name(
                base_model_name if base_model_name else "topic_model"
            )
            model_save_path = OUTPUT_PATH / "saved_models" / dynamic_model_name
            logger.debug(
                f"Saving the model in the following directory: {model_save_path}"
            )
            try:
                st.session_state["topic_model"].save(
                    model_save_path,
                    serialization=BERTOPIC_SERIALIZATION,
                    save_ctfidf=True,
                    save_embedding_model=True,
                )
                st.success(f"Model saved successfully as {model_save_path}")
                st.session_state["model_saved"] = True
                logger.success(f"Model saved successfully!")
            except Exception as e:
                st.error(f"Failed to save the model: {e}")
                logger.error(f"Failed to save the model: {e}")
        else:
            st.error("No model available to save. Please train a model first.")


def train_model():
    if (
        "time_filtered_df" in st.session_state
        and not st.session_state["time_filtered_df"].empty
    ):
        with st.spinner("Training model..."):
            full_dataset = st.session_state["time_filtered_df"]
            indices = full_dataset.index.tolist()

            form_parameters = ast.literal_eval(st.session_state["parameters"])

            (
                st.session_state["topic_model"],
                st.session_state["topics"],
                _,
                st.session_state["embeddings"],
                st.session_state["token_embeddings"],
                st.session_state["token_strings"],
            ) = train_BERTopic(
                full_dataset=full_dataset,
                indices=indices,
                form_parameters=form_parameters,
                cache_base_name=(
                    st.session_state["data_name"]
                    if st.session_state["split_method"] == "No split"
                    else f'{st.session_state["data_name"]}_split_by_paragraphs'
                ),
            )

        st.success("Model trained successfully!")
        st.info(
            "Embeddings aren't saved in cache and thus aren't loaded. Please make sure to train the model without "
            "using cached embeddings if you want correct and functional temporal visualizations."
        )

        temp = st.session_state["topic_model"].get_topic_info()
        st.session_state["topics_info"] = temp[
            temp["Topic"] != -1
        ]  # exclude -1 topic from topic list

        # TOPIC MODEL COHERENCE AND DIVERSITY METRICS (optional) :
        coherence_score_type = "c_npmi"
        diversity_score_type = "puw"
        logger.info(
            f"Calculating {coherence_score_type} coherence and {diversity_score_type} diversity..."
        )

        try:
            coherence = get_coherence_value(
                st.session_state["topic_model"],
                st.session_state["topics"],
                st.session_state["time_filtered_df"][TEXT_COLUMN],
                coherence_score_type,
            )
            logger.success(f"Coherence score [{coherence_score_type}]: {coherence}")

        except IndexError as e:
            logger.error(
                "Error while calculating coherence metric. This likely happens when you're using an LLM to represent "
                "the topics instead of keywords."
            )
        try:
            diversity = get_diversity_value(
                st.session_state["topic_model"],
                st.session_state["topics"],
                st.session_state["time_filtered_df"][TEXT_COLUMN],
                diversity_score_type="puw",
            )
            logger.success(f"Diversity score [{diversity_score_type}]: {diversity}")
        except IndexError as e:
            logger.error(
                "Error while calculating diversity metric. This likely happens when you're using an LLM to represent "
                "the topics instead of keywords."
            )

        st.session_state["model_trained"] = True
        if not st.session_state["model_saved"]:
            st.warning("Don't forget to save your model!", icon="⚠️")
    else:
        st.error(
            "No data available for training. Please ensure data is correctly loaded."
        )


################################################
################## MAIN PAGE ###################
################################################

# Wide layout
st.set_page_config(page_title="BERTrend - Topic Analysis", layout="wide")

restore_widget_state()

### TITLE ###
st.title(":part_alternation_mark: Topic analysis demo")

if "model_trained" not in st.session_state:
    st.session_state["model_trained"] = False
if "model_saved" not in st.session_state:
    st.session_state["model_saved"] = False


def apply_changes():
    # Update other parameters
    parameters = {
        **embedding_model_options,
        **bertopic_options,
        **umap_options,
        **hdbscan_options,
        **countvectorizer_options,
        **ctfidf_options,
        **representation_model_options,
    }
    st.session_state["parameters"] = str(parameters)

    save_widget_state()
    st.sidebar.success("Changes applied successfully!")


# In the sidebar form
with st.sidebar.form("parameters_sidebar"):
    st.title("Parameters")

    with st.expander("Embedding model"):
        embedding_model_options = embedding_model_options()

    with st.expander("Topics"):
        bertopic_options = bertopic_options()

    with st.expander("UMAP"):
        umap_options = umap_options()

    with st.expander("HDBSCAN"):
        hdbscan_options = hdbscan_options()

    with st.expander("Count Vectorizer"):
        countvectorizer_options = countvectorizer_options()

    with st.expander("c-TF-IDF"):
        ctfidf_options = ctfidf_options()

    with st.expander("Representation Models"):
        representation_model_options = representation_model_options()

        # Form submit button for applying changes
    # (using on_click with callback function causes a glitch where the button has to be clicked twice for changes to take effect)
    changes_applied = st.form_submit_button(
        label="Apply Changes", type="primary", use_container_width=True
    )
    if changes_applied:
        apply_changes()

# Separate button for training the model
if st.sidebar.button(
    "Train Model",
    type="primary",
    key="train_model_button",
    use_container_width=True,
    disabled=("parameters" not in st.session_state),
    help="Make sure to review and apply changes before clicking on this button.",
):
    train_model()

if "parameters" in st.session_state:
    st.sidebar.write(f"Current parameters:")
    st.sidebar.write(st.session_state["parameters"])

# Load selected DataFrame
if "language" not in st.session_state:
    st.session_state["language"] = "fr"
display_data_loading_component()

# Data overview
if "time_filtered_df" not in st.session_state:
    st.stop()
data_overview(st.session_state["time_filtered_df"])

# Save the model button
save_model_interface()

# TODO: Investigate the potentially deprecated save_model_interface() I implemented a while ago
# to save a BERTopic model to either load it up later or load it up somewhere else
